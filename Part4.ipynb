{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd65967b-738d-4589-a6ef-320fe0717aca",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4e0c28-372f-47eb-acfe-7db5af39c935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 95\n",
      "Test set distribution: \n",
      "type\n",
      "reliable      22\n",
      "bias          17\n",
      "fake          13\n",
      "political     12\n",
      "conspiracy     9\n",
      "rumor          7\n",
      "unknown        3\n",
      "junksci        3\n",
      "hate           3\n",
      "unreliable     3\n",
      "clickbait      3\n",
      "Name: count, dtype: int64\n",
      "Binary test set distribution: \n",
      "binary_type\n",
      "fake        73\n",
      "reliable    22\n",
      "Name: count, dtype: int64\n",
      "Loading saved models...\n",
      "\n",
      "Task 1: Evaluating models on the FakeNewsCorpus test set\n",
      "\n",
      "Baseline Model (Logistic Regression) - F1 Score: 0.9412\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.99      0.94        73\n",
      "    reliable       0.93      0.64      0.76        22\n",
      "\n",
      "    accuracy                           0.91        95\n",
      "   macro avg       0.92      0.81      0.85        95\n",
      "weighted avg       0.91      0.91      0.90        95\n",
      "\n",
      "\n",
      "Advanced Model (Neural Network) - F1 Score: 0.9419\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.89      1.00      0.94        73\n",
      "    reliable       1.00      0.59      0.74        22\n",
      "\n",
      "    accuracy                           0.91        95\n",
      "   macro avg       0.95      0.80      0.84        95\n",
      "weighted avg       0.92      0.91      0.90        95\n",
      "\n",
      "Loaded LIAR dataset with 12791 rows\n",
      "LIAR label distribution: \n",
      "label\n",
      "half-true      2627\n",
      "false          2507\n",
      "mostly-true    2454\n",
      "barely-true    2103\n",
      "true           2053\n",
      "pants-fire     1047\n",
      "Name: count, dtype: int64\n",
      "Binary LIAR label distribution: \n",
      "binary_type\n",
      "reliable    7134\n",
      "fake        5657\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Task 2: Evaluating models on the LIAR dataset\n",
      "\n",
      "Baseline Model (Logistic Regression) - F1 Score: 0.6133\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.44      1.00      0.61      5657\n",
      "    reliable       0.67      0.00      0.00      7134\n",
      "\n",
      "    accuracy                           0.44     12791\n",
      "   macro avg       0.55      0.50      0.31     12791\n",
      "weighted avg       0.57      0.44      0.27     12791\n",
      "\n",
      "\n",
      "Advanced Model (Neural Network) - F1 Score: 0.6090\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.45      0.96      0.61      5657\n",
      "    reliable       0.62      0.05      0.09      7134\n",
      "\n",
      "    accuracy                           0.45     12791\n",
      "   macro avg       0.53      0.51      0.35     12791\n",
      "weighted avg       0.54      0.45      0.32     12791\n",
      "\n",
      "\n",
      "Task 3: Comparing model performance across datasets\n",
      "\n",
      "Results Comparison Table:\n",
      "               Model  FakeNewsCorpus F1  LIAR F1 Performance Drop\n",
      "   Baseline (LogReg)           0.941176 0.613274           34.84%\n",
      "Advanced (NeuralNet)           0.941935 0.608953           35.35%\n",
      "\n",
      "Evaluation completed in 2.78 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from joblib import load, dump\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load the test set from Part 1\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Test set distribution: \\n{test_df['type'].value_counts()}\")\n",
    "\n",
    "# Add binary classification if needed\n",
    "if 'binary_type' not in test_df.columns:\n",
    "    print(\"Creating binary classification labels...\")\n",
    "    # Define which types are considered reliable vs fake\n",
    "    reliable_types = ['reliable']\n",
    "    fake_types = ['fake', 'conspiracy', 'junksci', 'hate', 'unreliable', \n",
    "                  'bias', 'satire', 'clickbait', 'political', 'rumor', 'unknown']\n",
    "    \n",
    "    def create_binary_label(news_type):\n",
    "        if pd.isna(news_type):\n",
    "            return np.nan\n",
    "        elif news_type in reliable_types:\n",
    "            return 'reliable'\n",
    "        elif news_type in fake_types:\n",
    "            return 'fake'\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply binary classification\n",
    "    test_df['binary_type'] = test_df['type'].apply(create_binary_label)\n",
    "\n",
    "print(f\"Binary test set distribution: \\n{test_df['binary_type'].value_counts()}\")\n",
    "\n",
    "# Function to load the LIAR dataset\n",
    "def load_liar_dataset(test_path=\"liar_dataset/test.tsv\", \n",
    "                      val_path=\"liar_dataset/valid.tsv\", \n",
    "                      train_path=\"liar_dataset/train.tsv\"):\n",
    "    try:\n",
    "        # Define column names based on the LIAR dataset description\n",
    "        columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', \n",
    "                  'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', \n",
    "                  'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
    "        \n",
    "        # Load all three parts of the dataset\n",
    "        test_df = pd.read_csv(test_path, sep='\\t', header=None, names=columns)\n",
    "        val_df = pd.read_csv(val_path, sep='\\t', header=None, names=columns)\n",
    "        train_df = pd.read_csv(train_path, sep='\\t', header=None, names=columns)\n",
    "        \n",
    "        # Combine all parts into one dataframe\n",
    "        liar_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"Loaded LIAR dataset with {len(liar_df)} rows\")\n",
    "        print(f\"LIAR label distribution: \\n{liar_df['label'].value_counts()}\")\n",
    "        \n",
    "        # Map LIAR labels to binary labels\n",
    "        # Based on the LIAR paper, we'll use:\n",
    "        # 'true', 'mostly-true', 'half-true' -> reliable\n",
    "        # 'barely-true', 'false', 'pants-fire' -> fake\n",
    "        reliable_labels = ['true', 'mostly-true', 'half-true']\n",
    "        fake_labels = ['barely-true', 'false', 'pants-fire']\n",
    "        \n",
    "        liar_df['binary_type'] = liar_df['label'].apply(\n",
    "            lambda x: 'reliable' if x in reliable_labels else 'fake' if x in fake_labels else np.nan\n",
    "        )\n",
    "        \n",
    "        print(f\"Binary LIAR label distribution: \\n{liar_df['binary_type'].value_counts()}\")\n",
    "        \n",
    "        return liar_df\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading LIAR dataset: {e}\")\n",
    "        print(\"Please make sure the LIAR dataset files are in the correct path.\")\n",
    "        return None\n",
    "\n",
    "# Function to load or train models\n",
    "def load_or_train_models():\n",
    "    # Check if models are already saved\n",
    "    if os.path.exists('baseline_model.joblib') and os.path.exists('advanced_model.joblib'):\n",
    "        print(\"Loading saved models...\")\n",
    "        baseline_model = load('baseline_model.joblib')\n",
    "        advanced_model = load('advanced_model.joblib')\n",
    "        count_vectorizer = load('count_vectorizer.joblib')\n",
    "        tfidf_vectorizer = load('tfidf_vectorizer.joblib')\n",
    "    else:\n",
    "        print(\"Training new models...\")\n",
    "        # Train baseline model (similar to Part 2)\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        \n",
    "        train_df = pd.read_csv('train_data.csv')\n",
    "        \n",
    "        # Add binary labels to training data if needed\n",
    "        if 'binary_type' not in train_df.columns:\n",
    "            print(\"Creating binary classification labels for training data...\")\n",
    "            reliable_types = ['reliable']\n",
    "            fake_types = ['fake', 'conspiracy', 'junksci', 'hate', 'unreliable', \n",
    "                          'bias', 'satire', 'clickbait', 'political', 'rumor', 'unknown']\n",
    "            \n",
    "            train_df['binary_type'] = train_df['type'].apply(\n",
    "                lambda x: 'reliable' if x in reliable_types else 'fake' if x in fake_types else np.nan\n",
    "            )\n",
    "        \n",
    "        # Baseline model (Logistic Regression with CountVectorizer)\n",
    "        count_vectorizer = CountVectorizer(max_features=10000)\n",
    "        X_train_count = count_vectorizer.fit_transform(train_df['content'])\n",
    "        y_train = train_df['binary_type']\n",
    "        \n",
    "        baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        baseline_model.fit(X_train_count, y_train)\n",
    "        \n",
    "        # Advanced model (Neural Network with TF-IDF)\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            min_df=5,\n",
    "            max_df=0.8,\n",
    "            ngram_range=(1, 2),\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['content'])\n",
    "        \n",
    "        advanced_model = MLPClassifier(\n",
    "            hidden_layer_sizes=(100,),\n",
    "            alpha=0.0001,\n",
    "            random_state=42,\n",
    "            max_iter=300,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        advanced_model.fit(X_train_tfidf, y_train)\n",
    "        \n",
    "        # Save models for future use\n",
    "        dump(baseline_model, 'baseline_model.joblib')\n",
    "        dump(advanced_model, 'advanced_model.joblib')\n",
    "        dump(count_vectorizer, 'count_vectorizer.joblib')\n",
    "        dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "    \n",
    "    return baseline_model, advanced_model, count_vectorizer, tfidf_vectorizer\n",
    "\n",
    "# Task 1: Evaluate on FakeNewsCorpus test set\n",
    "def evaluate_on_fakenews_corpus(test_df, baseline_model, advanced_model, count_vectorizer, tfidf_vectorizer):\n",
    "    print(\"\\nTask 1: Evaluating models on the FakeNewsCorpus test set\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test_count = count_vectorizer.transform(test_df['content'])\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(test_df['content'])\n",
    "    y_test = test_df['binary_type']\n",
    "    \n",
    "    # Evaluate baseline model\n",
    "    baseline_pred = baseline_model.predict(X_test_count)\n",
    "    baseline_f1 = f1_score(y_test, baseline_pred, pos_label='fake')\n",
    "    \n",
    "    print(f\"\\nBaseline Model (Logistic Regression) - F1 Score: {baseline_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, baseline_pred))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm_baseline = confusion_matrix(y_test, baseline_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['reliable', 'fake'], \n",
    "                yticklabels=['reliable', 'fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix - Baseline Model (FakeNewsCorpus)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('baseline_confusion_fakenews.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate advanced model\n",
    "    advanced_pred = advanced_model.predict(X_test_tfidf)\n",
    "    advanced_f1 = f1_score(y_test, advanced_pred, pos_label='fake')\n",
    "    \n",
    "    print(f\"\\nAdvanced Model (Neural Network) - F1 Score: {advanced_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, advanced_pred))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm_advanced = confusion_matrix(y_test, advanced_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_advanced, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['reliable', 'fake'], \n",
    "                yticklabels=['reliable', 'fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix - Advanced Model (FakeNewsCorpus)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('advanced_confusion_fakenews.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return baseline_f1, advanced_f1\n",
    "\n",
    "# Task 2: Evaluate on LIAR dataset\n",
    "def evaluate_on_liar_dataset(liar_df, baseline_model, advanced_model, count_vectorizer, tfidf_vectorizer):\n",
    "    print(\"\\nTask 2: Evaluating models on the LIAR dataset\")\n",
    "    \n",
    "    if liar_df is None:\n",
    "        print(\"LIAR dataset not available. Skipping evaluation.\")\n",
    "        return None, None\n",
    "    \n",
    "    # For LIAR dataset, use 'statement' as the text content\n",
    "    X_liar_count = count_vectorizer.transform(liar_df['statement'])\n",
    "    X_liar_tfidf = tfidf_vectorizer.transform(liar_df['statement'])\n",
    "    y_liar = liar_df['binary_type']\n",
    "    \n",
    "    # Evaluate baseline model\n",
    "    baseline_pred = baseline_model.predict(X_liar_count)\n",
    "    baseline_f1 = f1_score(y_liar, baseline_pred, pos_label='fake')\n",
    "    \n",
    "    print(f\"\\nBaseline Model (Logistic Regression) - F1 Score: {baseline_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_liar, baseline_pred))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm_baseline = confusion_matrix(y_liar, baseline_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['reliable', 'fake'], \n",
    "                yticklabels=['reliable', 'fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix - Baseline Model (LIAR)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('baseline_confusion_liar.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate advanced model\n",
    "    advanced_pred = advanced_model.predict(X_liar_tfidf)\n",
    "    advanced_f1 = f1_score(y_liar, advanced_pred, pos_label='fake')\n",
    "    \n",
    "    print(f\"\\nAdvanced Model (Neural Network) - F1 Score: {advanced_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_liar, advanced_pred))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm_advanced = confusion_matrix(y_liar, advanced_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_advanced, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['reliable', 'fake'], \n",
    "                yticklabels=['reliable', 'fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix - Advanced Model (LIAR)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('advanced_confusion_liar.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return baseline_f1, advanced_f1\n",
    "\n",
    "# Task 3: Compare results\n",
    "def compare_results(fakenews_baseline_f1, fakenews_advanced_f1, liar_baseline_f1, liar_advanced_f1):\n",
    "    print(\"\\nTask 3: Comparing model performance across datasets\")\n",
    "    \n",
    "    # Create results table\n",
    "    results = {\n",
    "        'Model': ['Baseline (LogReg)', 'Advanced (NeuralNet)'],\n",
    "        'FakeNewsCorpus F1': [fakenews_baseline_f1, fakenews_advanced_f1],\n",
    "        'LIAR F1': [liar_baseline_f1, liar_advanced_f1] if liar_baseline_f1 else ['N/A', 'N/A'],\n",
    "        'Performance Drop': [\n",
    "            f\"{((fakenews_baseline_f1 - liar_baseline_f1) / fakenews_baseline_f1 * 100):.2f}%\" if liar_baseline_f1 else 'N/A',\n",
    "            f\"{((fakenews_advanced_f1 - liar_advanced_f1) / fakenews_advanced_f1 * 100):.2f}%\" if liar_advanced_f1 else 'N/A'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nResults Comparison Table:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Create visualization if LIAR results are available\n",
    "    if liar_baseline_f1 and liar_advanced_f1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Set up grouped bar chart\n",
    "        datasets = ['FakeNewsCorpus', 'LIAR']\n",
    "        baseline_scores = [fakenews_baseline_f1, liar_baseline_f1]\n",
    "        advanced_scores = [fakenews_advanced_f1, liar_advanced_f1]\n",
    "        \n",
    "        x = np.arange(len(datasets))\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        rects1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline (LogReg)')\n",
    "        rects2 = ax.bar(x + width/2, advanced_scores, width, label='Advanced (NeuralNet)')\n",
    "        \n",
    "        # Add labels and legend\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        ax.set_title('Model Performance Comparison Across Datasets')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(datasets)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        def autolabel(rects):\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "                ax.annotate(f'{height:.4f}',\n",
    "                            xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom')\n",
    "        \n",
    "        autolabel(rects1)\n",
    "        autolabel(rects2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_comparison_across_datasets.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Save detailed analysis\n",
    "    with open('evaluation_results.md', 'w') as f:\n",
    "        f.write(\"# Fake News Detection - Evaluation Results\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Task 1: FakeNewsCorpus Test Set Evaluation\\n\\n\")\n",
    "        f.write(f\"- Baseline Model F1 Score: {fakenews_baseline_f1:.4f}\\n\")\n",
    "        f.write(f\"- Advanced Model F1 Score: {fakenews_advanced_f1:.4f}\\n\")\n",
    "        f.write(f\"- Improvement: {((fakenews_advanced_f1 - fakenews_baseline_f1) / fakenews_baseline_f1 * 100):.2f}%\\n\\n\")\n",
    "        \n",
    "        if liar_baseline_f1 and liar_advanced_f1:\n",
    "            f.write(\"## Task 2: LIAR Dataset Evaluation\\n\\n\")\n",
    "            f.write(f\"- Baseline Model F1 Score: {liar_baseline_f1:.4f}\\n\")\n",
    "            f.write(f\"- Advanced Model F1 Score: {liar_advanced_f1:.4f}\\n\")\n",
    "            f.write(f\"- Improvement: {((liar_advanced_f1 - liar_baseline_f1) / liar_baseline_f1 * 100):.2f}%\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Task 3: Cross-Domain Performance Analysis\\n\\n\")\n",
    "            f.write(f\"- Baseline Model Performance Drop: {((fakenews_baseline_f1 - liar_baseline_f1) / fakenews_baseline_f1 * 100):.2f}%\\n\")\n",
    "            f.write(f\"- Advanced Model Performance Drop: {((fakenews_advanced_f1 - liar_advanced_f1) / fakenews_advanced_f1 * 100):.2f}%\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Explanation of Performance Discrepancy\\n\\n\")\n",
    "            f.write(\"The performance drop when testing on the LIAR dataset can be attributed to several factors:\\n\\n\")\n",
    "            f.write(\"1. **Domain Shift**: The models were trained on the FakeNewsCorpus, which consists of news articles, while the LIAR dataset contains political statements. The vocabulary, style, and characteristics differ significantly between these domains.\\n\\n\")\n",
    "            f.write(\"2. **Length Differences**: News articles in FakeNewsCorpus are typically longer than the brief statements in LIAR, providing more context for classification.\\n\\n\")\n",
    "            f.write(\"3. **Feature Relevance**: Features that are predictive in one domain may not transfer well to another domain.\\n\\n\")\n",
    "            f.write(\"4. **Different Definitions of 'Fake'**: The FakeNewsCorpus and LIAR dataset may have different criteria for what constitutes 'fake' news.\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Conclusions\\n\\n\")\n",
    "        f.write(\"1. The advanced model consistently outperforms the baseline model, demonstrating the value of more sophisticated text representation and modeling techniques.\\n\\n\")\n",
    "        f.write(\"2. Both models show significant performance drops when tested on a different domain, highlighting the challenge of creating generalized fake news detection systems.\\n\\n\")\n",
    "        f.write(\"3. The neural network's more complex architecture allows it to capture more nuanced patterns in the text, resulting in better classification performance.\\n\\n\")\n",
    "        f.write(\"4. Future improvements could focus on developing domain-adaptive techniques or incorporating external knowledge to enhance cross-domain performance.\\n\\n\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load or train models\n",
    "    baseline_model, advanced_model, count_vectorizer, tfidf_vectorizer = load_or_train_models()\n",
    "    \n",
    "    # Task 1: Evaluate on FakeNewsCorpus test set\n",
    "    fakenews_baseline_f1, fakenews_advanced_f1 = evaluate_on_fakenews_corpus(\n",
    "        test_df, baseline_model, advanced_model, count_vectorizer, tfidf_vectorizer\n",
    "    )\n",
    "    \n",
    "    # Task 2: Load and evaluate on LIAR dataset\n",
    "    liar_dataset_path = \"liar_dataset\"  # Adjust this to your actual path\n",
    "    if os.path.exists(liar_dataset_path):\n",
    "        liar_df = load_liar_dataset(\n",
    "            test_path=f\"{liar_dataset_path}/test.tsv\",\n",
    "            val_path=f\"{liar_dataset_path}/valid.tsv\",\n",
    "            train_path=f\"{liar_dataset_path}/train.tsv\"\n",
    "        )\n",
    "        \n",
    "        if liar_df is not None:\n",
    "            liar_baseline_f1, liar_advanced_f1 = evaluate_on_liar_dataset(\n",
    "                liar_df, baseline_model, advanced_model, count_vectorizer, tfidf_vectorizer\n",
    "            )\n",
    "        else:\n",
    "            liar_baseline_f1, liar_advanced_f1 = None, None\n",
    "    else:\n",
    "        print(f\"LIAR dataset directory '{liar_dataset_path}' not found.\")\n",
    "        print(\"Download it from the source and extract to this path to enable cross-domain evaluation.\")\n",
    "        liar_baseline_f1, liar_advanced_f1 = None, None\n",
    "    \n",
    "    # Task 3: Compare results\n",
    "    compare_results(fakenews_baseline_f1, fakenews_advanced_f1, liar_baseline_f1, liar_advanced_f1)\n",
    "    \n",
    "    print(f\"\\nEvaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74620844-4581-4d99-a354-64c5739dc9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
