{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd65967b-738d-4589-a6ef-320fe0717aca",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe8bbdf-73f3-49b8-8464-0b8690b83d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from joblib import load, dump\n",
    "import time\n",
    "import os\n",
    "\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "# print(f\"Test set size: {len(test_df)}\")\n",
    "# print(f\"Test set distribution: \\n{test_df['type'].value_counts()}\")\n",
    "\n",
    "plots_dir = os.path.join(os.getcwd(), 'plots')\n",
    "models_dir = os.path.join(os.getcwd(), 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Add and then apply binary classification\n",
    "if 'binary_type' not in test_df.columns:\n",
    "    reliable_types = ['reliable']\n",
    "    fake_types = ['fake', 'conspiracy', 'junksci', 'hate', 'unreliable', \n",
    "                  'bias', 'satire', 'clickbait', 'political', 'rumor', 'unknown']\n",
    "    def create_binary_label(news_type):\n",
    "        if pd.isna(news_type):\n",
    "            return np.nan\n",
    "        elif news_type in reliable_types:\n",
    "            return 'reliable'\n",
    "        elif news_type in fake_types:\n",
    "            return 'fake'\n",
    "        else:\n",
    "            return np.nan\n",
    "    test_df['binary_type'] = test_df['type'].apply(create_binary_label)\n",
    "\n",
    "#print(f\"Binary test set distribution: \\n{test_df['binary_type'].value_counts()}\")\n",
    "\n",
    "if os.path.exists(os.path.join(models_dir, 'baseline_model.joblib')) and os.path.exists(os.path.join(models_dir, 'advanced_model.joblib')):\n",
    "    baseline_model = load(os.path.join(models_dir, 'baseline_model.joblib'))\n",
    "    advanced_model = load(os.path.join(models_dir, 'advanced_model.joblib'))\n",
    "    count_vectorizer = load(os.path.join(models_dir, 'count_vectorizer.joblib'))\n",
    "    tfidf_vectorizer = load(os.path.join(models_dir, 'tfidf_vectorizer.joblib'))\n",
    "else:\n",
    "    # Train baseline model\n",
    "    train_df = pd.read_csv('train_data.csv')\n",
    "    # Add binary labels to training data if needed\n",
    "    if 'binary_type' not in train_df.columns:\n",
    "        print(\"Creating binary classification labels for training data...\")\n",
    "        reliable_types = ['reliable']\n",
    "        fake_types = ['fake', 'conspiracy', 'junksci', 'hate', 'unreliable', \n",
    "                      'bias', 'satire', 'clickbait', 'political', 'rumor', 'unknown']\n",
    "        \n",
    "        train_df['binary_type'] = train_df['type'].apply(lambda x: 'reliable' if x in reliable_types else 'fake' if x in fake_types else np.nan)\n",
    "    # Baseline model (Logistic Regression with CountVectorizer)\n",
    "    count_vectorizer = CountVectorizer(max_features=10000)\n",
    "    X_train_count = count_vectorizer.fit_transform(train_df['content'])\n",
    "    y_train = train_df['binary_type']\n",
    "    baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    baseline_model.fit(X_train_count, y_train)\n",
    "    # Advanced model (Neural Network with TF-IDF)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=10000, min_df=5, max_df=0.8, ngram_range=(1, 2), sublinear_tf=True)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['content'])\n",
    "    advanced_model = MLPClassifier(hidden_layer_sizes=(100,), alpha=0.0001, random_state=42, max_iter=300, early_stopping=True)\n",
    "    advanced_model.fit(X_train_tfidf, y_train)\n",
    "    # Save models for future use\n",
    "    dump(baseline_model, os.path.join(models_dir, 'baseline_model.joblib'))\n",
    "    dump(advanced_model, os.path.join(models_dir, 'advanced_model.joblib'))\n",
    "    dump(count_vectorizer, os.path.join(models_dir, 'count_vectorizer.joblib'))\n",
    "    dump(tfidf_vectorizer, os.path.join(models_dir, 'tfidf_vectorizer.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b733824-c6bc-4ad1-b9a6-36d007bf23e2",
   "metadata": {},
   "source": [
    "# Task 1: Evaluate on FakeNewsCorpus test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4e0c28-372f-47eb-acfe-7db5af39c935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Model (Logistic Regression) - F1 Score: 0.9412\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.99      0.94        73\n",
      "    reliable       0.93      0.64      0.76        22\n",
      "\n",
      "    accuracy                           0.91        95\n",
      "   macro avg       0.92      0.81      0.85        95\n",
      "weighted avg       0.91      0.91      0.90        95\n",
      "\n",
      "\n",
      "Advanced Model (Neural Network) - F1 Score: 0.9419\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.89      1.00      0.94        73\n",
      "    reliable       1.00      0.59      0.74        22\n",
      "\n",
      "    accuracy                           0.91        95\n",
      "   macro avg       0.95      0.80      0.84        95\n",
      "weighted avg       0.92      0.91      0.90        95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_count = count_vectorizer.transform(test_df['content'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['content'])\n",
    "y_test = test_df['binary_type']\n",
    "\n",
    "# Evaluate baseline model\n",
    "baseline_pred = baseline_model.predict(X_test_count)\n",
    "fakenews_baseline_f1 = f1_score(y_test, baseline_pred, pos_label='fake')\n",
    "print(f\"\\nBaseline Model (Logistic Regression) - F1 Score: {fakenews_baseline_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, baseline_pred))\n",
    "\n",
    "# Evaluate advanced model\n",
    "advanced_pred = advanced_model.predict(X_test_tfidf)\n",
    "fakenews_advanced_f1 = f1_score(y_test, advanced_pred, pos_label='fake')\n",
    "print(f\"\\nAdvanced Model (Neural Network) - F1 Score: {fakenews_advanced_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, advanced_pred))\n",
    "\n",
    "# Create confusion matrices\n",
    "cm_baseline = confusion_matrix(y_test, baseline_pred)\n",
    "cm_advanced = confusion_matrix(y_test, advanced_pred)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', xticklabels=['reliable', 'fake'], yticklabels=['reliable', 'fake'], ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix - Baseline Model (FakeNewsCorpus)', fontsize=14)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "sns.heatmap(cm_advanced, annot=True, fmt='d', cmap='Blues', xticklabels=['reliable', 'fake'], yticklabels=['reliable', 'fake'], ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix - Advanced Model (FakeNewsCorpus)', fontsize=14)\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'Part4Task1_confusion_matrices_fakenews.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95820f01-7fe5-44e3-9bc9-2c2f56b04be2",
   "metadata": {},
   "source": [
    "# Task 2: Evaluate on LIAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206e4c63-7840-4571-9f3d-9a6e3e0612f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LIAR dataset with 12791 rows\n",
      "LIAR label distribution: \n",
      "label\n",
      "half-true      2627\n",
      "false          2507\n",
      "mostly-true    2454\n",
      "barely-true    2103\n",
      "true           2053\n",
      "pants-fire     1047\n",
      "Name: count, dtype: int64\n",
      "Binary LIAR label distribution: \n",
      "binary_type\n",
      "reliable    7134\n",
      "fake        5657\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Baseline Model (Logistic Regression) - F1 Score: 0.6133\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.44      1.00      0.61      5657\n",
      "    reliable       0.67      0.00      0.00      7134\n",
      "\n",
      "    accuracy                           0.44     12791\n",
      "   macro avg       0.55      0.50      0.31     12791\n",
      "weighted avg       0.57      0.44      0.27     12791\n",
      "\n",
      "\n",
      "Advanced Model (Neural Network) - F1 Score: 0.6090\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.45      0.96      0.61      5657\n",
      "    reliable       0.62      0.05      0.09      7134\n",
      "\n",
      "    accuracy                           0.45     12791\n",
      "   macro avg       0.53      0.51      0.35     12791\n",
      "weighted avg       0.54      0.45      0.32     12791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define column names based on the LIAR dataset description\n",
    "columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', \n",
    "          'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', \n",
    "          'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
    "# Load the dataset combine it into one dataframe and map LIAR labels to binary labels based on the LIAR paper\n",
    "test_df = pd.read_csv(\"liar_dataset/test.tsv\", sep='\\t', header=None, names=columns)\n",
    "val_df = pd.read_csv(\"liar_dataset/valid.tsv\", sep='\\t', header=None, names=columns)\n",
    "train_df = pd.read_csv(\"liar_dataset/train.tsv\", sep='\\t', header=None, names=columns)\n",
    "liar_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "print(f\"Loaded LIAR dataset with {len(liar_df)} rows\")\n",
    "print(f\"LIAR label distribution: \\n{liar_df['label'].value_counts()}\")\n",
    "reliable_labels = ['true', 'mostly-true', 'half-true']\n",
    "fake_labels = ['barely-true', 'false', 'pants-fire']\n",
    "liar_df['binary_type'] = liar_df['label'].apply(lambda x: 'reliable' if x in reliable_labels else 'fake' if x in fake_labels else np.nan)\n",
    "print(f\"Binary LIAR label distribution: \\n{liar_df['binary_type'].value_counts()}\")\n",
    "\n",
    "X_liar_count = count_vectorizer.transform(liar_df['statement'])\n",
    "X_liar_tfidf = tfidf_vectorizer.transform(liar_df['statement'])\n",
    "y_liar = liar_df['binary_type']\n",
    "\n",
    "# Evaluate baseline model\n",
    "baseline_pred = baseline_model.predict(X_liar_count)\n",
    "liar_baseline_f1 = f1_score(y_liar, baseline_pred, pos_label='fake')\n",
    "print(f\"\\nBaseline Model (Logistic Regression) - F1 Score: {liar_baseline_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_liar, baseline_pred))\n",
    "\n",
    "# Evaluate advanced model\n",
    "advanced_pred = advanced_model.predict(X_liar_tfidf)\n",
    "liar_advanced_f1 = f1_score(y_liar, advanced_pred, pos_label='fake')\n",
    "print(f\"\\nAdvanced Model (Neural Network) - F1 Score: {liar_advanced_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_liar, advanced_pred))\n",
    "\n",
    "# Create confusion matrices\n",
    "cm_baseline = confusion_matrix(y_liar, baseline_pred)\n",
    "cm_advanced = confusion_matrix(y_liar, advanced_pred)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', xticklabels=['reliable', 'fake'], yticklabels=['reliable', 'fake'], ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix - Baseline Model (LIAR)', fontsize=14)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "sns.heatmap(cm_advanced, annot=True, fmt='d', cmap='Blues', xticklabels=['reliable', 'fake'], yticklabels=['reliable', 'fake'], ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix - Advanced Model (LIAR)', fontsize=14)\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'Part4Task2_confusion_matrices_liar.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e30180-b2b9-4760-b887-cac6283539d8",
   "metadata": {},
   "source": [
    "# Task 3: Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7835d74-f464-43a7-b58a-1977fb0a80e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Comparison Table:\n",
      "               Model  FakeNewsCorpus F1  LIAR F1 Performance Drop\n",
      "   Baseline (LogReg)           0.941176 0.613274           34.84%\n",
      "Advanced (NeuralNet)           0.941935 0.608953           35.35%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compare_results(fakenews_baseline_f1, fakenews_advanced_f1, liar_baseline_f1, liar_advanced_f1):\n",
    "    results = {\n",
    "        'Model': ['Baseline (LogReg)', 'Advanced (NeuralNet)'],\n",
    "        'FakeNewsCorpus F1': [fakenews_baseline_f1, fakenews_advanced_f1],\n",
    "        'LIAR F1': [liar_baseline_f1, liar_advanced_f1] if liar_baseline_f1 else ['N/A', 'N/A'],\n",
    "        'Performance Drop': [\n",
    "            f\"{((fakenews_baseline_f1 - liar_baseline_f1) / fakenews_baseline_f1 * 100):.2f}%\" if liar_baseline_f1 else 'N/A',\n",
    "            f\"{((fakenews_advanced_f1 - liar_advanced_f1) / fakenews_advanced_f1 * 100):.2f}%\" if liar_advanced_f1 else 'N/A'\n",
    "        ]\n",
    "    }\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nResults Comparison Table:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    # Create visualization\n",
    "    if liar_baseline_f1 and liar_advanced_f1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # Set up grouped bar chart\n",
    "        datasets = ['FakeNewsCorpus', 'LIAR']\n",
    "        baseline_scores = [fakenews_baseline_f1, liar_baseline_f1]\n",
    "        advanced_scores = [fakenews_advanced_f1, liar_advanced_f1]\n",
    "        x = np.arange(len(datasets))\n",
    "        width = 0.35\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        rects1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline (LogReg)')\n",
    "        rects2 = ax.bar(x + width/2, advanced_scores, width, label='Advanced (NeuralNet)')\n",
    "        # Add labels and legend\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        ax.set_title('Model Performance Comparison Across Datasets')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(datasets)\n",
    "        ax.legend()\n",
    "        # Add value labels on bars\n",
    "        def autolabel(rects):\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "                ax.annotate(f'{height:.4f}', xy=(rect.get_x() + rect.get_width()/2, height), xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "        autolabel(rects1)\n",
    "        autolabel(rects2)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, 'Part4Task3_model_comparison_across_datasets.png'))\n",
    "        plt.close()\n",
    "\n",
    "compare_results(fakenews_baseline_f1, fakenews_advanced_f1, liar_baseline_f1, liar_advanced_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74620844-4581-4d99-a354-64c5739dc9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
