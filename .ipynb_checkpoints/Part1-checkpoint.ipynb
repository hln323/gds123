{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1570a1-2fe8-4c6a-a390-628d1d8ef791",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499ebb14-3c46-482c-b3ac-f67372da2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "plots_dir = os.path.join(os.getcwd(), 'plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8199c4-3730-45c1-aeff-38f2c59cee9f",
   "metadata": {},
   "source": [
    "# Task 1: Retrieve and Process the Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8f73ba-28b9-46c4-8f11-f1ee78c01d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded sample dataset with 250 rows\n",
      "Original vocabulary size: 16895\n",
      "Vocabulary after stopword removal: 16749\n",
      "Reduction rate after stopword removal: 0.86%\n",
      "Vocabulary after stemming: 11587\n",
      "Reduction rate after stemming: 30.82%\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text using regular expressions\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text) # Replace URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text) # Replace emails\n",
    "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '<DATE>', text) # Replace dates\n",
    "    text = re.sub(r'\\d{1,2}-\\d{1,2}-\\d{2,4}', '<DATE>', text) # Replace numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '<NUM>', text)\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove excess whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Load sample dataset\n",
    "try:\n",
    "    url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "    sample_df = pd.read_csv(url)\n",
    "    print(f\"Successfully loaded sample dataset with {len(sample_df)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading from URL: {e}\")\n",
    "    sample_df = pd.read_csv(\"news_sample.csv\")\n",
    "    print(f\"Loaded from local file: {len(sample_df)} rows\")\n",
    "\n",
    "# Clean and tokenize the sample\n",
    "sample_df['cleaned_content'] = sample_df['content'].apply(clean_text)\n",
    "sample_df['tokens'] = sample_df['cleaned_content'].apply(word_tokenize)\n",
    "\n",
    "# Calculate vocabulary size\n",
    "all_tokens = [token for tokens_list in sample_df['tokens'] for token in tokens_list]\n",
    "vocab_size_original = len(set(all_tokens))\n",
    "print(f\"Original vocabulary size: {vocab_size_original}\")\n",
    "\n",
    "# Remove stopwords and calculate vocabulary size and reduction rate after stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sample_df['tokens_no_stopwords'] = sample_df['tokens'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "all_tokens_no_stopwords = [token for tokens_list in sample_df['tokens_no_stopwords'] for token in tokens_list]\n",
    "vocab_size_no_stopwords = len(set(all_tokens_no_stopwords))\n",
    "stopword_reduction_rate = ((vocab_size_original - vocab_size_no_stopwords) / vocab_size_original) * 100\n",
    "print(f\"Vocabulary after stopword removal: {vocab_size_no_stopwords}\")\n",
    "print(f\"Reduction rate after stopword removal: {stopword_reduction_rate:.2f}%\")\n",
    "\n",
    "# Apply stemming and calculate vocabulary size and reduction rate after stopword stemming\n",
    "ps = PorterStemmer()\n",
    "sample_df['tokens_stemmed'] = sample_df['tokens_no_stopwords'].apply(lambda tokens: [ps.stem(token) for token in tokens])\n",
    "all_tokens_stemmed = [token for tokens_list in sample_df['tokens_stemmed'] for token in tokens_list]\n",
    "vocab_size_stemmed = len(set(all_tokens_stemmed))\n",
    "stemming_reduction_rate = ((vocab_size_no_stopwords - vocab_size_stemmed) / vocab_size_no_stopwords) * 100\n",
    "print(f\"Vocabulary after stemming: {vocab_size_stemmed}\")\n",
    "print(f\"Reduction rate after stemming: {stemming_reduction_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38b569-e9ce-4c4b-a31c-96317912ed30",
   "metadata": {},
   "source": [
    "# Task 2: Apply preprocessing to the full dataset and make observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277ab7c0-fc70-4aeb-b9c0-075b4c53ba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded in 10.84 seconds\n",
      "Dataset shape: (1003, 17)\n",
      "\n",
      "Missing values per column:\n",
      "                  Missing Values  Percentage\n",
      "Unnamed: 0                     0         0.0\n",
      "id                             0         0.0\n",
      "domain                         0         0.0\n",
      "type                          57         5.7\n",
      "url                            0         0.0\n",
      "content                        0         0.0\n",
      "scraped_at                     0         0.0\n",
      "inserted_at                    0         0.0\n",
      "updated_at                     0         0.0\n",
      "title                         10         1.0\n",
      "authors                      471        47.0\n",
      "keywords                    1003       100.0\n",
      "meta_keywords                 33         3.3\n",
      "meta_description             554        55.2\n",
      "tags                         767        76.5\n",
      "summary                     1003       100.0\n",
      "source                       783        78.1\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Try to load with appropriate sampling or load the whole set\n",
    "    skip = sorted(np.random.choice(np.arange(1, 995000), 995000 - SAMPLE_SIZE, replace=False))\n",
    "    large_df = pd.read_csv(\"995,000_rows.csv\", skiprows=skip)\n",
    "    # large_df = pd.read_csv(\"995,000_rows.csv\")\n",
    "    print(f\"Dataset loaded in {time.time() - start_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the full dataset: {e}\")\n",
    "    print(\"Using the sample dataset instead for demonstration...\")\n",
    "    large_df = sample_df.copy()\n",
    "\n",
    "print(f\"Dataset shape: {large_df.shape}\")\n",
    "\n",
    "# Display missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_values = large_df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(large_df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage.round(1)\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Apply cleaning to the large dataset\n",
    "large_df['cleaned_content'] = large_df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfafc2-d12f-4c65-b27f-8b633d3add80",
   "metadata": {},
   "source": [
    "# Task 3: Complete preprocessing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9131281-b7c1-4d33-be3e-a1caacc57b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average URLs per article: 0.26\n",
      "  Maximum URLs in an article: 32\n",
      "  Average dates per article: 0.04\n",
      "  Maximum dates in an article: 2\n",
      "  Average numbers per article: 8.74\n",
      "  Maximum numbers in an article: 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|███████████████████████████████████████████████████| 1003/1003 [00:05<00:00, 193.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed in 5.20 seconds\n",
      "\n",
      "Vocabulary Statistics after full preprocessing:\n",
      "Original vocabulary size: 32744\n",
      "Vocabulary after stopword removal: 32595\n",
      "Vocabulary after stemming: 24100\n",
      "Reduction rate after stopword removal: 0.46%\n",
      "Reduction rate after stemming: 26.06%\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessing utilities\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "tqdm.pandas(desc=\"Processing articles\")\n",
    "\n",
    "# Function to count patterns in text\n",
    "def count_patterns(text):\n",
    "    if not isinstance(text, str):\n",
    "        return (0, 0, 0)\n",
    "    urls = len(re.findall(r'<URL>', text))\n",
    "    dates = len(re.findall(r'<DATE>', text))\n",
    "    numbers = len(re.findall(r'<NUM>', text))\n",
    "    return (urls, dates, numbers)\n",
    "\n",
    "def compute_word_counts(column):\n",
    "    text = ' '.join(large_df[column].dropna().tolist())\n",
    "    tokens = word_tokenize(text)\n",
    "    return Counter(tokens)\n",
    "\n",
    "# Apply pattern counting\n",
    "large_df['url_count'], large_df['date_count'], large_df['number_count'] = zip(*large_df['cleaned_content'].apply(count_patterns))\n",
    "\n",
    "# Calculate statistics\n",
    "print(f\"  Average URLs per article: {large_df['url_count'].mean():.2f}\")\n",
    "print(f\"  Maximum URLs in an article: {large_df['url_count'].max()}\")\n",
    "print(f\"  Average dates per article: {large_df['date_count'].mean():.2f}\")\n",
    "print(f\"  Maximum dates in an article: {large_df['date_count'].max()}\")\n",
    "print(f\"  Average numbers per article: {large_df['number_count'].mean():.2f}\")\n",
    "print(f\"  Maximum numbers in an article: {large_df['number_count'].max()}\")\n",
    "\n",
    "# Word frequency, domain, article length, average length by type and news type distribution analysis\n",
    "type_counts = large_df['type'].value_counts()\n",
    "sample_text = ' '.join(large_df['cleaned_content'].dropna().sample(min(100, len(large_df))).tolist())\n",
    "sample_tokens = word_tokenize(sample_text)\n",
    "word_counts = Counter(sample_tokens)\n",
    "domain_counts = large_df['domain'].value_counts()\n",
    "domain_labels = [d.split('.')[0] for d in domain_counts.head(15).index]\n",
    "large_df['content_length'] = large_df['content'].apply(lambda x: len(str(x)) if isinstance(x, str) else 0)\n",
    "type_stats = large_df.groupby('type').agg(\n",
    "    count=('content', 'count'),\n",
    "    avg_length=('content_length', 'mean')\n",
    ")\n",
    "words_20, counts_20 = zip(*word_counts.most_common(20))\n",
    "top_10000_words = word_counts.most_common(10000)\n",
    "_, counts_10k = zip(*top_10000_words)\n",
    "\n",
    "# Complete preprocessing pipeline\n",
    "def preprocess_text(text):\n",
    "    cleaned_text = clean_text(text) # 1. Clean the text\n",
    "    tokens = word_tokenize(cleaned_text) # 2. Tokenize\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words] # 3. Remove stopwords\n",
    "    stemmed_tokens = [ps.stem(token) for token in filtered_tokens] # 4. Apply stemming\n",
    "    processed_text = ' '.join(stemmed_tokens) # 5. Join tokens back into text\n",
    "    return cleaned_text, ' '.join(tokens), ' '.join(filtered_tokens), processed_text\n",
    "\n",
    "# Apply preprocessing and create new columns\n",
    "start_time = time.time()\n",
    "results = large_df['content'].progress_apply(lambda x: preprocess_text(x) if isinstance(x, str) else (\"\", \"\", \"\", \"\"))\n",
    "large_df['cleaned_content'], large_df['tokenized_content'], large_df['no_stopwords_content'], large_df['stemmed_content'] = zip(*results)\n",
    "print(f\"Preprocessing completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "raw_counts = compute_word_counts('tokenized_content')\n",
    "no_stop_counts = compute_word_counts('no_stopwords_content')\n",
    "stemmed_counts = compute_word_counts('stemmed_content')\n",
    "\n",
    "raw_freq = [count for _, count in raw_counts.most_common(100)]\n",
    "no_stop_freq = [count for _, count in no_stop_counts.most_common(100)]\n",
    "stemmed_freq = [count for _, count in stemmed_counts.most_common(100)]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# 1. News type distribution\n",
    "type_counts.plot(kind='bar', color='skyblue', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribution of News Types', fontsize=18)\n",
    "axes[0, 0].set_xlabel('Type', fontsize=16)\n",
    "axes[0, 0].set_ylabel('Count', fontsize=16)\n",
    "axes[0, 0].tick_params(axis='x', rotation=30, labelsize=14)\n",
    "axes[0, 0].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# 2. Top 20 most common words\n",
    "axes[0, 1].bar(words_20, counts_20, color='skyblue')\n",
    "axes[0, 1].set_title('Top 20 Most Common Words', fontsize=18)\n",
    "axes[0, 1].set_xlabel('Words', fontsize=16)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=16)\n",
    "axes[0, 1].tick_params(axis='x', rotation=30, labelsize=14)\n",
    "axes[0, 1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# 3. Top 15 domains\n",
    "domain_counts.head(15).plot(kind='bar', color='lightgreen', ax=axes[1, 0])\n",
    "axes[1, 0].set_xticklabels(domain_labels, rotation=30, ha='right', fontsize=14)\n",
    "axes[1, 0].set_title('Top 15 Most Common Domains', fontsize=18)\n",
    "axes[1, 0].set_xlabel('Domain', fontsize=16)\n",
    "axes[1, 0].set_ylabel('Count', fontsize=16)\n",
    "axes[1, 0].tick_params(axis='x', rotation=30, labelsize=14)\n",
    "axes[1, 0].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# 4. Article length by type\n",
    "sns.boxplot(x='type', y='content_length', data=large_df, ax=axes[1, 1], width=0.6, fliersize=3)\n",
    "axes[1, 1].set_title('Article Length by News Type', fontsize=18)\n",
    "axes[1, 1].set_xlabel('News Type', fontsize=16)\n",
    "axes[1, 1].set_ylabel('Content Length (chars)', fontsize=16)\n",
    "axes[1, 1].tick_params(axis='x', rotation=30, labelsize=14)\n",
    "axes[1, 1].tick_params(axis='y', labelsize=14)\n",
    "axes[1, 1].set_ylim(0, large_df['content_length'].quantile(0.95))\n",
    "\n",
    "# 5. Top 10,000 words plot\n",
    "axes[2, 0].plot(counts_10k, color='steelblue')\n",
    "axes[2, 0].set_title('Frequency of the 10,000 Most Frequent Words', fontsize=18)\n",
    "axes[2, 0].set_xlabel('Word Rank', fontsize=16)\n",
    "axes[2, 0].set_ylabel('Frequency', fontsize=16)\n",
    "axes[2, 0].tick_params(axis='x', labelsize=14)\n",
    "axes[2, 0].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# 6. Comparison: Raw vs. No-Stopwords vs. Stemmed\n",
    "axes[2, 1].plot(raw_freq, label='Raw', color='blue')\n",
    "axes[2, 1].plot(no_stop_freq, label='No Stopwords', color='orange')\n",
    "axes[2, 1].plot(stemmed_freq, label='Stemmed', color='green')\n",
    "axes[2, 1].set_title('Top 100 Word Frequency Comparison', fontsize=18)\n",
    "axes[2, 1].set_xlabel('Rank', fontsize=16)\n",
    "axes[2, 1].set_ylabel('Frequency', fontsize=16)\n",
    "axes[2, 1].legend(fontsize=14)\n",
    "axes[2, 1].tick_params(axis='x', labelsize=14)\n",
    "axes[2, 1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'Part1Task3_combined_3x2_plots.png'))\n",
    "plt.close()\n",
    "\n",
    "# Calculate vocabulary sizes after full preprocessing\n",
    "def get_vocab_sizes(df):\n",
    "    # Create lists of all tokens\n",
    "    all_tokens = ' '.join(df['tokenized_content'].dropna().tolist()).split()\n",
    "    all_no_stopwords = ' '.join(df['no_stopwords_content'].dropna().tolist()).split()\n",
    "    all_stemmed = ' '.join(df['stemmed_content'].dropna().tolist()).split()\n",
    "    # Get unique tokens\n",
    "    vocab_original = len(set(all_tokens))\n",
    "    vocab_no_stopwords = len(set(all_no_stopwords))\n",
    "    vocab_stemmed = len(set(all_stemmed))\n",
    "    # Calculate reduction rates\n",
    "    stopword_reduction = ((vocab_original - vocab_no_stopwords) / vocab_original) * 100\n",
    "    stemming_reduction = ((vocab_no_stopwords - vocab_stemmed) / vocab_no_stopwords) * 100\n",
    "    return {\n",
    "        'vocab_original': vocab_original,\n",
    "        'vocab_no_stopwords': vocab_no_stopwords,\n",
    "        'vocab_stemmed': vocab_stemmed,\n",
    "        'stopword_reduction': stopword_reduction,\n",
    "        'stemming_reduction': stemming_reduction\n",
    "    }\n",
    "\n",
    "# Display vocabulary statistics\n",
    "vocab_stats = get_vocab_sizes(large_df)\n",
    "print(\"\\nVocabulary Statistics after full preprocessing:\")\n",
    "print(f\"Original vocabulary size: {vocab_stats['vocab_original']}\")\n",
    "print(f\"Vocabulary after stopword removal: {vocab_stats['vocab_no_stopwords']}\")\n",
    "print(f\"Vocabulary after stemming: {vocab_stats['vocab_stemmed']}\")\n",
    "print(f\"Reduction rate after stopword removal: {vocab_stats['stopword_reduction']:.2f}%\")\n",
    "print(f\"Reduction rate after stemming: {vocab_stats['stemming_reduction']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37c48d-2a9b-4afa-8fca-f8e706776401",
   "metadata": {},
   "source": [
    "# Task 4: Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134b17f4-4512-4fa6-972e-6f38978f8ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary class distribution:\n",
      "binary_type\n",
      "fake        726\n",
      "reliable    220\n",
      "Name: count, dtype: int64\n",
      "After removing rows with missing binary_type: 946 rows\n",
      "Training set size: 756 (79.9%)\n",
      "Validation set size: 95 (10.0%)\n",
      "Test set size: 95 (10.0%)\n",
      "\n",
      "Training set binary distribution:\n",
      "binary_type\n",
      "fake        580\n",
      "reliable    176\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set binary distribution:\n",
      "binary_type\n",
      "fake        73\n",
      "reliable    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set binary distribution:\n",
      "binary_type\n",
      "fake        73\n",
      "reliable    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set original type distribution:\n",
      "type\n",
      "reliable      176\n",
      "political     149\n",
      "bias          108\n",
      "fake           84\n",
      "conspiracy     81\n",
      "rumor          39\n",
      "unreliable     34\n",
      "unknown        31\n",
      "clickbait      24\n",
      "junksci        12\n",
      "satire          9\n",
      "hate            9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Binary classification approach\n",
    "def create_binary_label(news_type):\n",
    "    if pd.isna(news_type):\n",
    "        return np.nan\n",
    "    reliable_types = ['reliable']\n",
    "    fake_types = ['fake', 'conspiracy', 'junksci', 'hate', 'unreliable', \n",
    "                 'bias', 'satire', 'clickbait', 'political', 'rumor', 'unknown']\n",
    "    if news_type in reliable_types:\n",
    "        return 'reliable'\n",
    "    elif news_type in fake_types:\n",
    "        return 'fake'\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply and check binary classification\n",
    "large_df['binary_type'] = large_df['type'].apply(create_binary_label)\n",
    "print(\"Binary class distribution:\")\n",
    "binary_counts = large_df['binary_type'].value_counts()\n",
    "print(binary_counts)\n",
    "\n",
    "# Remove rows with missing binary_type\n",
    "df_clean = large_df.dropna(subset=['binary_type'])\n",
    "print(f\"After removing rows with missing binary_type: {len(df_clean)} rows\")\n",
    "\n",
    "# Data splits. First training vs. rest (80% / 20%) then validation vs. test (50% / 50% of the remaining 20%)\n",
    "train_df, temp_df = train_test_split(df_clean, test_size=0.2, random_state=42, stratify=df_clean['binary_type'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['binary_type'])\n",
    "print(f\"Training set size: {len(train_df)} ({len(train_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(val_df)} ({len(val_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(test_df)} ({len(test_df)/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each split and original distribution in each split for reference\n",
    "print(\"\\nTraining set binary distribution:\")\n",
    "print(train_df['binary_type'].value_counts())\n",
    "print(\"\\nValidation set binary distribution:\")\n",
    "print(val_df['binary_type'].value_counts())\n",
    "print(\"\\nTest set binary distribution:\")\n",
    "print(test_df['binary_type'].value_counts())\n",
    "print(\"\\nTraining set original type distribution:\")\n",
    "print(train_df['type'].value_counts())\n",
    "\n",
    "output_dir = os.getcwd()\n",
    "train_df.to_csv(os.path.join(output_dir, 'train_data.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(output_dir, 'validation_data.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(output_dir, 'test_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a66fb-99f3-42b2-9cec-e77373e94d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
